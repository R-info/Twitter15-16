{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "feff3561",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '../..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8a6e983",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>label</th>\n",
       "      <th>tvt2</th>\n",
       "      <th>tvt2_1</th>\n",
       "      <th>tvt2_2</th>\n",
       "      <th>tvt2_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>656955120626880512</td>\n",
       "      <td>correct predictions in back to the future ii U...</td>\n",
       "      <td>False</td>\n",
       "      <td>training</td>\n",
       "      <td>training</td>\n",
       "      <td>training</td>\n",
       "      <td>training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>615689290706595840</td>\n",
       "      <td>.@whitehouse in rainbow colors for #scotusmarr...</td>\n",
       "      <td>True</td>\n",
       "      <td>testting</td>\n",
       "      <td>testting</td>\n",
       "      <td>training</td>\n",
       "      <td>testting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>613404935003217920</td>\n",
       "      <td>cops bought the alleged church shooter burger ...</td>\n",
       "      <td>False</td>\n",
       "      <td>validation</td>\n",
       "      <td>training</td>\n",
       "      <td>training</td>\n",
       "      <td>training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>614467824313106432</td>\n",
       "      <td>god put a rainbow over the white house ðŸŒˆ URL\\r</td>\n",
       "      <td>True</td>\n",
       "      <td>validation</td>\n",
       "      <td>training</td>\n",
       "      <td>training</td>\n",
       "      <td>training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>622891631293935616</td>\n",
       "      <td>#wakeupamericaðŸ‡ºðŸ‡¸ who needs a #gun registry whe...</td>\n",
       "      <td>False</td>\n",
       "      <td>training</td>\n",
       "      <td>testting</td>\n",
       "      <td>training</td>\n",
       "      <td>training</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id                                         tweet_text  \\\n",
       "0  656955120626880512  correct predictions in back to the future ii U...   \n",
       "1  615689290706595840  .@whitehouse in rainbow colors for #scotusmarr...   \n",
       "2  613404935003217920  cops bought the alleged church shooter burger ...   \n",
       "3  614467824313106432     god put a rainbow over the white house ðŸŒˆ URL\\r   \n",
       "4  622891631293935616  #wakeupamericaðŸ‡ºðŸ‡¸ who needs a #gun registry whe...   \n",
       "\n",
       "   label        tvt2    tvt2_1    tvt2_2    tvt2_3  \n",
       "0  False    training  training  training  training  \n",
       "1   True    testting  testting  training  testting  \n",
       "2  False  validation  training  training  training  \n",
       "3   True  validation  training  training  training  \n",
       "4  False    training  testting  training  training  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"../../data/processed/twitter16-tf_dataset.csv\", lineterminator=\"\\n\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b95fb4a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[False, True]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# labels_str = ['rumour', 'non-rumour']\n",
    "label_type = \"label\"\n",
    "labels_str = data[label_type].unique().tolist()\n",
    "labels_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6ce7b52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 0, 1, 0, 1, 1, 0, 0, 1]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = []\n",
    "for i, d in data.iterrows():\n",
    "    lab = labels_str.index(d[label_type])\n",
    "    labels.append(lab)\n",
    "labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fc83a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tokenizer = TweetTokenizer(reduce_len=True)\n",
    "\n",
    "\n",
    "def text2unigrams(text):\n",
    "    texts = tokenizer.tokenize(text.encode('ascii', 'ignore').decode('utf8'))\n",
    "    texts = [t for t in texts if t not in string.punctuation]\n",
    "    texts = [t for t in texts if t not in ['URL', 'â€˜', 'â€™']]\n",
    "    \n",
    "    unigrams = texts\n",
    "    \n",
    "    return unigrams\n",
    "\n",
    "\n",
    "def text2bigrams(text):\n",
    "    texts = tokenizer.tokenize(text.encode('ascii', 'ignore').decode('utf8'))\n",
    "    texts = [t for t in texts if t not in string.punctuation]\n",
    "    texts = [t for t in texts if t not in ['URL', 'â€˜', 'â€™']]\n",
    "    \n",
    "    bigrams = nltk.bigrams(texts)\n",
    "    bigrams = map(' '.join, bigrams)\n",
    "    bigrams = [bgr for bgr in bigrams]\n",
    "    \n",
    "    return bigrams\n",
    "\n",
    "\n",
    "def text2trigrams(text):\n",
    "    texts = tokenizer.tokenize(text.encode('ascii', 'ignore').decode('utf8'))\n",
    "    texts = [t for t in texts if t not in string.punctuation]\n",
    "    texts = [t for t in texts if t not in ['URL', 'â€˜', 'â€™']]\n",
    "    \n",
    "    trigrams = nltk.trigrams(texts)\n",
    "    trigrams = map(' '.join, trigrams)\n",
    "    trigrams = [bgr for bgr in trigrams]\n",
    "    \n",
    "    return trigrams\n",
    "\n",
    "\n",
    "def custom_vectors_generation(texts, vector_terms):\n",
    "    vectors = []\n",
    "    for text in texts:\n",
    "        bigrams = text2bigrams(text)\n",
    "        trigrams = text2trigrams(text)\n",
    "\n",
    "        init_vec = [0.0 for _ in range(len(vector_terms) + 1)]\n",
    "        for bgr in bigrams:\n",
    "            if bgr in vector_terms:\n",
    "                idx = vector_terms.index(bgr)\n",
    "                init_vec[idx] = 1.0\n",
    "            else:\n",
    "                init_vec[-1] = 1.0\n",
    "        for tgr in trigrams:\n",
    "            if tgr in vector_terms:\n",
    "                idx = vector_terms.index(tgr)\n",
    "                init_vec[idx] = 1.0\n",
    "            else:\n",
    "                init_vec[-1] = 1.0\n",
    "        vectors.append(init_vec)\n",
    "    \n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97281e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = data['tweet_text'].tolist()\n",
    "# vectors = bigrams_vectors_generation(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31e1c006",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.collocations import *\n",
    "\n",
    "top_n = 2000\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "\n",
    "finder2 = BigramCollocationFinder.from_words([])\n",
    "finder3 = TrigramCollocationFinder.from_words([])\n",
    "\n",
    "# generating bigram and trigram\n",
    "for text in texts:\n",
    "    unigrams = text2unigrams(text)\n",
    "    bigrams = text2bigrams(text)\n",
    "    trigrams = text2trigrams(text)\n",
    "    \n",
    "    for ngrm in unigrams:\n",
    "        if ngrm not in finder2.word_fd:\n",
    "            finder2.word_fd[ngrm] = 0\n",
    "        finder2.word_fd[ngrm] += 1\n",
    "        finder2.N += 1\n",
    "        \n",
    "        if ngrm not in finder3.word_fd:\n",
    "            finder3.word_fd[ngrm] = 0\n",
    "        finder3.word_fd[ngrm] += 1\n",
    "        finder3.N += 1\n",
    "    \n",
    "    for ngrm in bigrams:\n",
    "        term = tuple([i for i in ngrm.split()])\n",
    "        \n",
    "        if term not in finder2.ngram_fd:\n",
    "            finder2.ngram_fd[term] = 0\n",
    "            \n",
    "        finder2.ngram_fd[term] += 1\n",
    "\n",
    "    for ngrm in trigrams:\n",
    "        term = tuple([i for i in ngrm.split()])\n",
    "        \n",
    "        if term not in finder3.ngram_fd:\n",
    "            finder3.ngram_fd[term] = 0\n",
    "            \n",
    "        finder3.ngram_fd[term] += 1\n",
    "        \n",
    "# only bigrams that appear 3+ times\n",
    "finder2.apply_freq_filter(3)\n",
    "finder3.apply_freq_filter(3)\n",
    "\n",
    "combined = []\n",
    "for res in finder2.score_ngrams(bigram_measures.pmi):\n",
    "    combined.append(res)\n",
    "for res in finder3.score_ngrams(trigram_measures.pmi):\n",
    "    combined.append(res)\n",
    "combined = sorted(combined, key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "703b0f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "296 178\n",
      "('kissing', 'islands', 'greenland') - 20.90396755446038\n",
      "('confiscates', 'several', 'thousand') - 20.73404255301807\n",
      "('fda', 'confiscates', 'several') - 20.73404255301807\n",
      "('nasa', 'confirms', 'earth') - 20.319005053739225\n",
      "('several', 'thousand', 'chickens') - 20.319005053739225\n",
      "('experience', '15', 'days') - 20.260111364685656\n",
      "('translucent', 'butterfly', 'beautiful') - 20.260111364685656\n",
      "('earth', 'will', 'experience') - 20.149080052296913\n",
      "('cpl', 'nathan', 'cirillo') - 19.51165013168162\n",
      "('sun', 'goes', 'down') - 19.45275644262805\n"
     ]
    }
   ],
   "source": [
    "print(len(finder2.ngram_fd), len(finder3.ngram_fd))\n",
    "count = 0\n",
    "for k, v in combined:\n",
    "    print(f\"{k} - {v}\")\n",
    "    count += 1\n",
    "    if count >= 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e3377a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "term_vector_base = [\" \".join(c[0]) for c in combined[:top_n]]\n",
    "vectors = custom_vectors_generation(texts, term_vector_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66934ad8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "475"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectors[102])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e2b1180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8762081973554245\n",
      "20.90396755446038\n",
      "296\n",
      "178\n"
     ]
    }
   ],
   "source": [
    "min_score = 100\n",
    "max_score = 0\n",
    "n_2gram = 0\n",
    "n_3gram = 0\n",
    "for k, v in combined[:top_n]:\n",
    "    min_score = min(min_score, v)\n",
    "    max_score = max(max_score, v)\n",
    "    \n",
    "    if len(k) == 2:\n",
    "        n_2gram += 1\n",
    "    \n",
    "    if len(k) == 3:\n",
    "        n_3gram += 1\n",
    "        \n",
    "print(min_score)\n",
    "print(max_score)\n",
    "print(n_2gram)\n",
    "print(n_3gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8cccd2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vectors = np.array([vectors[i] for i, d in data.iterrows() if d['tvt2'] == 'training'])\n",
    "val_vectors = np.array([vectors[i] for i, d in data.iterrows() if d['tvt2'] == 'validation'])\n",
    "test_vectors = np.array([vectors[i] for i, d in data.iterrows() if d['tvt2'] == 'testting'])\n",
    "\n",
    "train_labels = np.array([labels[i] for i, d in data.iterrows() if d['tvt2'] == 'training'])\n",
    "val_labels = np.array([labels[i] for i, d in data.iterrows() if d['tvt2'] == 'validation'])\n",
    "test_labels = np.array([labels[i] for i, d in data.iterrows() if d['tvt2'] == 'testting'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81824aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- START ---\n",
      "---> execution time : 0.01 seconds\n",
      "Validation Set\n",
      "Binary Class Evaluation\n",
      "\n",
      "True Positive : 30\n",
      "False Positive : 3\n",
      "False Negative : 17\n",
      "True Negative : 48\n",
      "\n",
      "Class positive Evaluation\n",
      "- Precision : 90.909 %\n",
      "- Recall : 63.83 %\n",
      "- F1 : 0.75\n",
      "\n",
      "Class negative Evaluation\n",
      "- Precision : 73.846 %\n",
      "- Recall : 94.118 %\n",
      "- F1 : 0.82759\n",
      "\n",
      "Combined Evaluation\n",
      "- Accuracy : 79.592 %\n",
      "- Precision : 82.378 %\n",
      "- Recall : 78.974 %\n",
      "- F1 : 0.8064\n",
      "- Average Confidence : 100.0 %\n",
      "Model, Combined,,,,positive,,,negative,,,\n",
      "Anonymous, 79.592, 82.378, 78.974, 0.8064, 90.909, 63.83, 0.75, 73.846, 94.118, 0.82759, \n",
      "--- END ---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from library.classification import SKLearnClassification\n",
    "from library.evaluation import ConfusionMatrix\n",
    "\n",
    "svm = LinearSVC()\n",
    "\n",
    "model = SKLearnClassification(svm, \"Support Vector Machine\")\n",
    "print(f\"\\n--- START ---\")\n",
    "model.train(train_vectors, train_labels, \"Phemernr2\")\n",
    "\n",
    "print(\"Validation Set\")\n",
    "preds = model.predict(val_vectors)\n",
    "\n",
    "conf_mat = ConfusionMatrix(\n",
    "    labels=val_labels,\n",
    "    predictions=preds,\n",
    "    binary=True\n",
    ")\n",
    "conf_mat.evaluate()\n",
    "\n",
    "print(\"--- END ---\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "33a6804c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens : 474\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "coefs = model.model.coef_\n",
    "if type(coefs) == csr_matrix:\n",
    "    coefs.toarray().tolist()[0]\n",
    "else:\n",
    "    coefs.tolist()\n",
    "coefs_and_features = list(zip(coefs[0], term_vector_base))\n",
    "\n",
    "# Most predictive overall\n",
    "# coefs_and_features = sorted(coefs_and_features, key=lambda x: x[0], reverse=True)\n",
    "coefs_and_features = sorted(coefs_and_features, key=lambda x: abs(x[0]), reverse=True)\n",
    "print(f\"Total tokens : {len(coefs_and_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c865ec2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 475)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "468e19ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(-0.9672015364260023, 'mass shootings'),\n",
       " (0.9627758825201459, 'charlie hebdo'),\n",
       " (0.8969884223655178, 'lindt cafe'),\n",
       " (-0.895821093986824, 'he was'),\n",
       " (0.8857540051797733, '#charliehebdo attackers'),\n",
       " (0.8857539324029002, 'will be'),\n",
       " (0.8384020753145732, 'a rainbow'),\n",
       " (0.8174717123117751, 'parliament hill'),\n",
       " (-0.7936067049757771, 'is not'),\n",
       " (-0.7652636963621833, 'red cross'),\n",
       " (-0.7652636416813171, 'el chapo'),\n",
       " (0.7479932528988155, 'in sydney'),\n",
       " (0.7435382648924156, 'the #sydneysiege'),\n",
       " (0.7412850873896215, 'sydney cafe'),\n",
       " (0.7381285207456704, 'to be'),\n",
       " (0.7381284293644936, 'watch live'),\n",
       " (0.7381282808282211, 'police confirm'),\n",
       " (0.7381282518596124, 'vatican says'),\n",
       " (0.738128222570653, 'to me'),\n",
       " (-0.7142462427476106, 'about that'),\n",
       " (-0.7142460534815773, '#opkkk #hoodsoff'),\n",
       " (-0.7142460131177104, 'in new'),\n",
       " (-0.6613389838600973, 'burger king'),\n",
       " (-0.6578582127574935, 'bernie sanders'),\n",
       " (-0.6291034095822957, 'the future'),\n",
       " (0.6118842972498638, 'dead in'),\n",
       " (0.6096017583859882, 'in ottawa'),\n",
       " (-0.5952050822740305, 'poll @hillaryclinton'),\n",
       " (-0.5952050105055275, 'red cups'),\n",
       " (0.5888620971101522, 'of the'),\n",
       " (-0.5855907632074495, 'donald trump'),\n",
       " (-0.5851457439577111, 'that he'),\n",
       " (0.5765361217761825, 'the gunman'),\n",
       " (-0.56135935832359, 'killed in'),\n",
       " (0.5579946204970124, 'white house'),\n",
       " (0.55424159799857, '#charliehebdo suspects'),\n",
       " (0.5532612128981435, 'prime minister'),\n",
       " (0.5339352535414944, 'gunman in'),\n",
       " (0.5298902393621049, 'by police'),\n",
       " (0.5278516318406687, 'hostage situation'),\n",
       " (0.5250851580740606, 'at least'),\n",
       " (-0.5183682261233352, 'this is'),\n",
       " (-0.4949313189987027, 'a new'),\n",
       " (-0.49388144384317945, 'look like'),\n",
       " (-0.4777982992356428, 'water with'),\n",
       " (0.47658052294738956, 'there is'),\n",
       " (-0.46673821551866473, 'translucent butterfly'),\n",
       " (0.4489274386925612, 'pope francis'),\n",
       " (0.4489274386925612, 'kim davis'),\n",
       " (0.4428770029480063, 'suspect in'),\n",
       " (0.44287700294800614, 'hostage-taker in'),\n",
       " (-0.44015799172592024, 'on the'),\n",
       " (0.4364405484121907, 'martin place'),\n",
       " (-0.4264798577290461, 'isis flag'),\n",
       " (0.39955346357591687, 'been killed'),\n",
       " (-0.3968034367675637, 'for me'),\n",
       " (-0.3968034367675636, 'arrested for'),\n",
       " (-0.38944620432698807, 'car dealership'),\n",
       " (0.38486823015244026, 'shot dead'),\n",
       " (0.3837370145954627, 'war memorial'),\n",
       " (0.3824703239956484, 'supermarket in paris'),\n",
       " (0.3824703239956484, 'supermarket in'),\n",
       " (0.37815188070010686, 'nathan cirillo'),\n",
       " (0.3660315773503156, 'have been killed'),\n",
       " (0.3660315773503156, 'killed by'),\n",
       " (-0.35843161837098086, 'adolf hitler'),\n",
       " (-0.35843161837098086, 'the same'),\n",
       " (-0.357123058612439, 'syrian refugees'),\n",
       " (-0.357123058612439, 'arrived in'),\n",
       " (-0.3571228268458684, 'rupert murdoch'),\n",
       " (-0.3571228268458684, 'national geographic'),\n",
       " (-0.35313617886873844, 'shot at'),\n",
       " (0.3522201467317783, '6 asparagus water'),\n",
       " (0.3522201467317783, 'whole foods'),\n",
       " (0.3522201467317783, '6 asparagus'),\n",
       " (0.3522201467317783, 'asparagus water'),\n",
       " (0.33686284187562, 'according to'),\n",
       " (0.328935570545575, 'shooting at'),\n",
       " (0.31132651140506723, 'soldier shot'),\n",
       " (-0.308638345852666, 'up in'),\n",
       " (0.3078352713950891, 'canadian soldier'),\n",
       " (0.2988902132668797, 'in paris'),\n",
       " (0.2942619736409317, 'hq of'),\n",
       " (0.29415145869068837, 'being held'),\n",
       " (-0.2869781053564366, 'steve jobs'),\n",
       " (-0.2747100476921349, 'open on sundays'),\n",
       " (-0.2747100476921349, 'on sundays'),\n",
       " (-0.2747100476921349, 'open on'),\n",
       " (0.2731616744904456, 'the canadian'),\n",
       " (-0.27024669414514335, 'in the'),\n",
       " (0.2687588533427647, 'soldier who'),\n",
       " (-0.25508782087839055, 'cnn orc poll'),\n",
       " (-0.25508782087839055, 'cnn orc'),\n",
       " (-0.25508782087839055, 'orc poll'),\n",
       " (0.2527308971473925, 'same-sex marriage'),\n",
       " (0.2527308971473925, 'marriage ruling'),\n",
       " (-0.24902441346865678, 'this morning'),\n",
       " (-0.24902441346865672, 'hit the'),\n",
       " (0.24393807041675578, 'islamic flag'),\n",
       " (0.24243611566129178, '#sydneysiege is'),\n",
       " (-0.22880828475831155, 'biological father was'),\n",
       " (-0.22880828475831155, 'biological father'),\n",
       " (-0.22880828475831155, 'father was'),\n",
       " (0.22657775495741359, 'was a refugee'),\n",
       " (0.22657775495741359, 'a refugee'),\n",
       " (0.20909571695429308, 'of her'),\n",
       " (-0.20744003228388053, 'have been'),\n",
       " (0.20450870136997829, 'shots fired'),\n",
       " (-0.19840175884102576, 'dylann roof'),\n",
       " (-0.19363725861086828, 'has been'),\n",
       " (-0.18795950100998615, 'new cnn'),\n",
       " (0.18764209614618002, 'in rainbow'),\n",
       " (0.1728504941246593, 'five hostages'),\n",
       " (0.1719230386730705, 'the @whitehouse is'),\n",
       " (0.1719230386730705, '@whitehouse is'),\n",
       " (0.1719230386730705, 'the @whitehouse'),\n",
       " (0.17162433949539074, 'breaking news'),\n",
       " (0.16135746003100004, 'the white house'),\n",
       " (0.16135746003100004, 'the white'),\n",
       " (0.15064012277436412, 'in martin place'),\n",
       " (0.15064012277436412, 'in martin'),\n",
       " (-0.15041068752730963, 'in a'),\n",
       " (-0.1454124579400011, 'watch list'),\n",
       " (-0.1454124579400011, 'employees on'),\n",
       " (0.14002822670264736, 'a syrian'),\n",
       " (0.13927424128080734, 'cpl nathan cirillo'),\n",
       " (0.13927424128080734, 'is cpl nathan'),\n",
       " (0.13927424128080734, 'cpl nathan'),\n",
       " (0.13927424128080734, 'is cpl'),\n",
       " (0.13094522867175837, 'confirmed dead'),\n",
       " (0.12922300200366144, 'pride ap photo'),\n",
       " (0.12922300200366144, 'pride ap'),\n",
       " (0.12922300200366144, 'ap photo'),\n",
       " (0.12922300200366138, 'with rainbow'),\n",
       " (0.12870440462870186, 'live updates'),\n",
       " (-0.1250449323822213, 'of a'),\n",
       " (-0.12501915893835816, 'kissing islands greenland'),\n",
       " (-0.12501915893835816, 'the kissing islands'),\n",
       " (-0.12501915893835816, 'islands greenland'),\n",
       " (-0.12501915893835816, 'kissing islands'),\n",
       " (-0.12501915893835816, 'the kissing'),\n",
       " (0.11968794932289614, 'rainbow colors'),\n",
       " (0.11116662081612828, 'gay marriage'),\n",
       " (0.10996109105405105, 'in rainbow colors'),\n",
       " (-0.10945140435764462, 'terrorist watch list'),\n",
       " (-0.10945140435764462, 'employees on terrorist'),\n",
       " (-0.10945140435764462, 'on terrorist watch'),\n",
       " (-0.10945140435764462, 'terrorist watch'),\n",
       " (-0.10945140435764462, 'on terrorist'),\n",
       " (0.10454489837179663, 'have escaped the'),\n",
       " (0.10454489837179663, 'have escaped'),\n",
       " (0.10454489837179663, 'escaped the'),\n",
       " (0.10199847579630915, 'is a'),\n",
       " (-0.10169492443590772, 'back to the'),\n",
       " (-0.10169492443590772, 'to the future'),\n",
       " (-0.10169492443590772, 'back to'),\n",
       " (0.10153733499505464, 'soldier killed'),\n",
       " (0.09734020517682633, 'in honor of'),\n",
       " (0.09734020517682633, 'marriage decision'),\n",
       " (0.09734020517682633, 'honor of'),\n",
       " (0.09734020517682633, 'in honor'),\n",
       " (0.09713451384737422, 'the soldier'),\n",
       " (0.09627761961454652, '11 dead'),\n",
       " (0.09053471674316689, 'colors to celebrate'),\n",
       " (0.09053471674316689, 'lights up'),\n",
       " (0.09053471674316689, 'to celebrate'),\n",
       " (0.09053471674316686, 'rainbow colors to'),\n",
       " (0.09053471674316686, 'colors to'),\n",
       " (-0.08751367806949274, 'translucent butterfly beautiful'),\n",
       " (-0.08751367806949274, 'butterfly beautiful'),\n",
       " (-0.08437067514179342, 'from the'),\n",
       " (-0.07818711350914459, 'is the'),\n",
       " (0.06598380076383872, 'to the'),\n",
       " (-0.05816982059812521, 'refugee from syria'),\n",
       " (-0.05816982059812521, 'founder of apple'),\n",
       " (-0.05816982059812521, 'of apple was'),\n",
       " (-0.05816982059812521, 'apple was a'),\n",
       " (-0.05816982059812521, 'father of steve'),\n",
       " (-0.05816982059812521, 'of steve jobs'),\n",
       " (-0.05816982059812521, 'a refugee from'),\n",
       " (-0.05816982059812521, 'steve jobs the'),\n",
       " (-0.05816982059812521, 'the father of'),\n",
       " (-0.05816982059812521, 'from syria'),\n",
       " (-0.05816982059812521, 'apple was'),\n",
       " (-0.05816982059812521, 'refugee from'),\n",
       " (-0.05816982059812521, 'founder of'),\n",
       " (-0.05816982059812521, 'of apple'),\n",
       " (-0.05816982059812521, 'father of'),\n",
       " (-0.05816982059812521, 'of steve'),\n",
       " (-0.05816982059812521, 'jobs the'),\n",
       " (-0.05816982059812521, 'the father'),\n",
       " (0.050329509091953474, 'over the'),\n",
       " (0.05032950909195346, 'rainbow over the'),\n",
       " (0.05032950909195346, 'over the white'),\n",
       " (0.05032950909195346, 'rainbow over'),\n",
       " (-0.04914535652848456, 'nasa confirms earth'),\n",
       " (-0.04914535652848456, 'experience 15 days'),\n",
       " (-0.04914535652848456, 'earth will experience'),\n",
       " (-0.04914535652848456, 'will experience 15'),\n",
       " (-0.04914535652848456, 'confirms earth will'),\n",
       " (-0.04914535652848456, 'darkness in november'),\n",
       " (-0.04914535652848456, '15 days of'),\n",
       " (-0.04914535652848456, 'in november 2015'),\n",
       " (-0.04914535652848456, 'experience 15'),\n",
       " (-0.04914535652848456, 'confirms earth'),\n",
       " (-0.04914535652848456, 'nasa confirms'),\n",
       " (-0.04914535652848456, 'november 2015'),\n",
       " (-0.04914535652848456, '15 days'),\n",
       " (-0.04914535652848456, 'earth will'),\n",
       " (-0.04914535652848456, 'will experience'),\n",
       " (-0.04914535652848456, 'days of'),\n",
       " (-0.04914535652848456, 'darkness in'),\n",
       " (-0.04914535652848456, 'in november'),\n",
       " (0.0441460894959573, 'hostages in'),\n",
       " (0.044146089495957296, 'kosher supermarket'),\n",
       " (0.043073804115762414, 'up rainbow'),\n",
       " (-0.03596105358235675, '72 dhs employees'),\n",
       " (-0.03596105358235675, 'dhs employees on'),\n",
       " (-0.03596105358235675, '72 dhs'),\n",
       " (-0.03596105358235675, 'dhs employees'),\n",
       " (0.03467359690464292, 'at war memorial'),\n",
       " (0.03467359690464292, 'at war'),\n",
       " (0.03408477186360623, 'at parliament hill'),\n",
       " (0.03408477186360623, 'at parliament'),\n",
       " (-0.033658996225661625, 'confiscates several thousand'),\n",
       " (-0.033658996225661625, 'fda confiscates several'),\n",
       " (-0.033658996225661625, 'several thousand chickens'),\n",
       " (-0.033658996225661625, 'as mutations worsen'),\n",
       " (-0.033658996225661625, 'farms as mutations'),\n",
       " (-0.033658996225661625, 'kfc farms as'),\n",
       " (-0.033658996225661625, 'from kfc farms'),\n",
       " (-0.033658996225661625, 'thousand chickens from'),\n",
       " (-0.033658996225661625, 'chickens from kfc'),\n",
       " (-0.033658996225661625, 'fda confiscates'),\n",
       " (-0.033658996225661625, 'mutations worsen'),\n",
       " (-0.033658996225661625, 'kfc farms'),\n",
       " (-0.033658996225661625, 'thousand chickens'),\n",
       " (-0.033658996225661625, 'confiscates several'),\n",
       " (-0.033658996225661625, 'several thousand'),\n",
       " (-0.033658996225661625, 'as mutations'),\n",
       " (-0.033658996225661625, 'farms as'),\n",
       " (-0.033658996225661625, 'chickens from'),\n",
       " (-0.033658996225661625, 'from kfc'),\n",
       " (0.03352188622560091, 'on parliament hill'),\n",
       " (0.03352188622560091, 'on parliament'),\n",
       " (0.03334694584691729, 'is lit up'),\n",
       " (0.03334694584691729, 'white house is'),\n",
       " (0.03334694584691729, 'is lit'),\n",
       " (0.03334694584691729, 'house is'),\n",
       " (0.03334694584691728, 'lit up'),\n",
       " (0.03298232486131858, 'a way to'),\n",
       " (0.03298232486131858, 'way to'),\n",
       " (0.03298232486131858, 'a way'),\n",
       " (0.030175008898878898, 'his partner met'),\n",
       " (0.030175008898878898, 'before kim davis'),\n",
       " (0.030175008898878898, 'openly gay man'),\n",
       " (0.030175008898878898, 'partner met with'),\n",
       " (0.030175008898878898, 'day before kim'),\n",
       " (0.030175008898878898, 'an openly gay'),\n",
       " (0.030175008898878898, 'gay man said'),\n",
       " (0.030175008898878898, 'kim davis did'),\n",
       " (0.030175008898878898, 'met with pope'),\n",
       " (0.030175008898878898, 'man said that'),\n",
       " (0.030175008898878898, 'with pope francis'),\n",
       " (0.030175008898878898, 'and his partner'),\n",
       " (0.030175008898878898, 'said that he'),\n",
       " (0.030175008898878898, 'a day before'),\n",
       " (0.030175008898878898, 'pope francis a'),\n",
       " (0.030175008898878898, 'francis a day'),\n",
       " (0.030175008898878898, 'he and his'),\n",
       " (0.030175008898878898, 'that he and'),\n",
       " (0.030175008898878898, 'partner met'),\n",
       " (0.030175008898878898, 'before kim'),\n",
       " (0.030175008898878898, 'openly gay'),\n",
       " (0.030175008898878898, 'an openly'),\n",
       " (0.030175008898878898, 'day before'),\n",
       " (0.030175008898878898, 'man said'),\n",
       " (0.030175008898878898, 'his partner'),\n",
       " (0.030175008898878898, 'davis did'),\n",
       " (0.030175008898878898, 'met with'),\n",
       " (0.030175008898878898, 'gay man'),\n",
       " (0.030175008898878898, 'said that'),\n",
       " (0.030175008898878898, 'with pope'),\n",
       " (0.030175008898878898, 'he and'),\n",
       " (0.030175008898878898, 'francis a'),\n",
       " (0.030175008898878898, 'and his'),\n",
       " (0.030175008898878898, 'a day'),\n",
       " (0.02959529741643359, 'in sydney cafe'),\n",
       " (0.015326816940427233, 'supreme court ruling'),\n",
       " (0.015326816940427233, 'white house lit'),\n",
       " (0.015326816940427233, 'house lit in'),\n",
       " (0.015326816940427233, 'lit in rainbow'),\n",
       " (0.015326816940427233, 'supreme court'),\n",
       " (0.015326816940427233, 'court ruling'),\n",
       " (0.015326816940427233, 'house lit'),\n",
       " (0.015326816940427233, 'lit in'),\n",
       " (0.013826415639301929, 'light up'),\n",
       " (0.011610222832516004, 'us supreme court'),\n",
       " (0.011610222832516004, 'following us supreme'),\n",
       " (0.011610222832516004, 'couples have right'),\n",
       " (0.011610222832516004, 'same-sex couples have'),\n",
       " (0.011610222832516004, 'that same-sex couples'),\n",
       " (0.011610222832516004, 'colors following us'),\n",
       " (0.011610222832516004, 'right to marry'),\n",
       " (0.011610222832516004, 'court ruling that'),\n",
       " (0.011610222832516004, 'ruling that same-sex'),\n",
       " (0.011610222832516004, 'rainbow colors following'),\n",
       " (0.011610222832516004, 'have right to'),\n",
       " (0.011610222832516004, 'same-sex couples'),\n",
       " (0.011610222832516004, 'following us'),\n",
       " (0.011610222832516004, 'us supreme'),\n",
       " (0.011610222832516004, 'couples have'),\n",
       " (0.011610222832516004, 'have right'),\n",
       " (0.011610222832516004, 'colors following'),\n",
       " (0.011610222832516004, 'that same-sex'),\n",
       " (0.011610222832516004, 'to marry'),\n",
       " (0.011610222832516004, 'right to'),\n",
       " (0.011610222832516004, 'ruling that'),\n",
       " (0.009726858268845019, 'sun goes down'),\n",
       " (0.009726858268845019, 'as sun goes'),\n",
       " (0.009726858268845019, 'celebrate scotus ruling'),\n",
       " (0.009726858268845019, 'goes down white'),\n",
       " (0.009726858268845019, 'to celebrate scotus'),\n",
       " (0.009726858268845019, 'house lights up'),\n",
       " (0.009726858268845019, 'lights up rainbow'),\n",
       " (0.009726858268845019, 'down white house'),\n",
       " (0.009726858268845019, 'white house lights'),\n",
       " (0.009726858268845019, 'up rainbow colors'),\n",
       " (0.009726858268845019, 'sun goes'),\n",
       " (0.009726858268845019, 'goes down'),\n",
       " (0.009726858268845019, 'celebrate scotus'),\n",
       " (0.009726858268845019, 'scotus ruling'),\n",
       " (0.009726858268845019, 'as sun'),\n",
       " (0.009726858268845019, 'down white'),\n",
       " (0.009726858268845019, 'house lights'),\n",
       " (-0.006201457901488872, 'riding escalators after'),\n",
       " (-0.006201457901488872, 'are riding escalators'),\n",
       " (-0.006201457901488872, 'china are riding'),\n",
       " (-0.006201457901488872, 'a horrific accident'),\n",
       " (-0.006201457901488872, 'is how people'),\n",
       " (-0.006201457901488872, 'this is how'),\n",
       " (-0.006201457901488872, 'after a horrific'),\n",
       " (-0.006201457901488872, 'escalators after a'),\n",
       " (-0.006201457901488872, 'people in china'),\n",
       " (-0.006201457901488872, 'in china are'),\n",
       " (-0.006201457901488872, 'how people in'),\n",
       " (-0.006201457901488872, 'horrific accident'),\n",
       " (-0.006201457901488872, 'riding escalators'),\n",
       " (-0.006201457901488872, 'escalators after'),\n",
       " (-0.006201457901488872, 'china are'),\n",
       " (-0.006201457901488872, 'how people'),\n",
       " (-0.006201457901488872, 'are riding'),\n",
       " (-0.006201457901488872, 'is how'),\n",
       " (-0.006201457901488872, 'a horrific'),\n",
       " (-0.006201457901488872, 'in china'),\n",
       " (-0.006201457901488872, 'after a'),\n",
       " (-0.006201457901488872, 'people in'),\n",
       " (0.003716594107911245, 'rainbow colors after'),\n",
       " (0.003716594107911245, 'colors after'),\n",
       " (-0.003280480409778084, 'future predictions that'),\n",
       " (-0.003280480409778084, '10 back to'),\n",
       " (-0.003280480409778084, 'the future predictions'),\n",
       " (-0.003280480409778084, 'came true'),\n",
       " (-0.003280480409778084, '10 back'),\n",
       " (-0.003280480409778084, 'future predictions'),\n",
       " (-0.003280480409778084, 'predictions that'),\n",
       " (-0.0022305298008979124, 'was a'),\n",
       " (7.049619646284808e-05, 'endorsement of'),\n",
       " (7.049619646282032e-05, 'with kim davis'),\n",
       " (7.049619646282032e-05, 'with kim'),\n",
       " (3.469446951953614e-18, 'pres obama'),\n",
       " (-8.673617379884035e-19, 'live as sun'),\n",
       " (-8.673617379884035e-19, 'watch live as'),\n",
       " (-8.673617379884035e-19, 'live as'),\n",
       " (0.0, 'bush jr 16'),\n",
       " (0.0, 'bush sr 12'),\n",
       " (0.0, 'that came true'),\n",
       " (0.0, 'jr 16 mass'),\n",
       " (0.0, 'clinton 23 mass'),\n",
       " (0.0, 'sr 12 mass'),\n",
       " (0.0, 'predictions that came'),\n",
       " (0.0, 'shootings clinton 23'),\n",
       " (0.0, 'adopted his biological'),\n",
       " (0.0, 'reagan 11 mass'),\n",
       " (0.0, 'shootings bush jr'),\n",
       " (0.0, 'shootings bush sr'),\n",
       " (0.0, 'refugees has arrived'),\n",
       " (0.0, '10,000 syrian refugees'),\n",
       " (0.0, 'his biological father'),\n",
       " (0.0, 'was abdulfattah jandali'),\n",
       " (0.0, 'obama 162 mass'),\n",
       " (0.0, 'shootings obama 162'),\n",
       " (0.0, 'gay marriage ruling'),\n",
       " (0.0, 'father was abdulfattah'),\n",
       " (0.0, 'jobs was adopted'),\n",
       " (0.0, 'load of 10,000'),\n",
       " (0.0, 'pattern now of'),\n",
       " (0.0, 'a pattern now'),\n",
       " (0.0, 'first load of'),\n",
       " (0.0, 'steve jobs was'),\n",
       " (0.0, '162 mass shootings'),\n",
       " (0.0, '23 mass shootings'),\n",
       " (0.0, 'mass shootings bush'),\n",
       " (0.0, 'was adopted his'),\n",
       " (0.0, 'abdulfattah jandali a'),\n",
       " (0.0, '16 mass shootings'),\n",
       " (0.0, 'syrian refugees has'),\n",
       " (0.0, '12 mass shootings'),\n",
       " (0.0, 'mass shootings clinton'),\n",
       " (0.0, 'have a pattern'),\n",
       " (0.0, 'jandali a syrian'),\n",
       " (0.0, '11 mass shootings'),\n",
       " (0.0, 'mass shootings obama'),\n",
       " (0.0, 'a syrian muslim'),\n",
       " (0.0, 'in new hampshire'),\n",
       " (0.0, 'has arrived in'),\n",
       " (0.0, 'arrived in new'),\n",
       " (0.0, 'of 10,000 syrian'),\n",
       " (0.0, 'we have a'),\n",
       " (0.0, 'in new orleans'),\n",
       " (0.0, 'in this country'),\n",
       " (0.0, 'like a rainbow'),\n",
       " (0.0, 'now of mass'),\n",
       " (0.0, '@berniesanders in new'),\n",
       " (0.0, 'of mass shootings'),\n",
       " (0.0, 'jr 16'),\n",
       " (0.0, 'in shooting at'),\n",
       " (0.0, 'shootings in this'),\n",
       " (0.0, 'clinton 23'),\n",
       " (0.0, 'sr 12'),\n",
       " (0.0, 'mass shootings in'),\n",
       " (0.0, 'bush jr'),\n",
       " (0.0, 'bush sr'),\n",
       " (0.0, 'pattern now'),\n",
       " (0.0, 'reagan 11'),\n",
       " (0.0, 'abdulfattah jandali'),\n",
       " (0.0, 'first load'),\n",
       " (0.0, 'obama 162'),\n",
       " (0.0, 'new hampshire'),\n",
       " (0.0, 'new orleans'),\n",
       " (0.0, 'adopted his'),\n",
       " (0.0, 'has arrived'),\n",
       " (0.0, 'his biological'),\n",
       " (0.0, '10,000 syrian'),\n",
       " (0.0, 'syrian muslim'),\n",
       " (0.0, 'this country'),\n",
       " (0.0, '162 mass'),\n",
       " (0.0, '23 mass'),\n",
       " (0.0, 'shootings bush'),\n",
       " (0.0, 'that came'),\n",
       " (0.0, '16 mass'),\n",
       " (0.0, 'we have'),\n",
       " (0.0, 'was abdulfattah'),\n",
       " (0.0, 'was adopted'),\n",
       " (0.0, 'vote for'),\n",
       " (0.0, '12 mass'),\n",
       " (0.0, 'jobs was'),\n",
       " (0.0, 'shootings clinton'),\n",
       " (0.0, '11 mass'),\n",
       " (0.0, 'refugees has'),\n",
       " (0.0, 'load of'),\n",
       " (0.0, 'jandali a'),\n",
       " (0.0, 'a crow'),\n",
       " (0.0, 'a pattern'),\n",
       " (0.0, 'shootings obama'),\n",
       " (0.0, 'of 10,000'),\n",
       " (0.0, 'like a'),\n",
       " (0.0, 'now of'),\n",
       " (0.0, '@berniesanders in'),\n",
       " (0.0, 'have a'),\n",
       " (0.0, 'in shooting'),\n",
       " (0.0, 'of mass'),\n",
       " (0.0, 'in this'),\n",
       " (0.0, 'shootings in')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefs_and_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5bc37609",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(-0.9672015364260023, 'mass shootings'),\n",
       " (0.9627758825201459, 'charlie hebdo'),\n",
       " (0.8969884223655178, 'lindt cafe'),\n",
       " (-0.895821093986824, 'he was'),\n",
       " (0.8857540051797733, '#charliehebdo attackers'),\n",
       " (0.8857539324029002, 'will be'),\n",
       " (0.8384020753145732, 'a rainbow'),\n",
       " (0.8174717123117751, 'parliament hill'),\n",
       " (-0.7936067049757771, 'is not'),\n",
       " (-0.7652636963621833, 'red cross')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefs_and_features[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a600a1af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.0, 'shootings obama'),\n",
       " (0.0, 'of 10,000'),\n",
       " (0.0, 'like a'),\n",
       " (0.0, 'now of'),\n",
       " (0.0, '@berniesanders in'),\n",
       " (0.0, 'have a'),\n",
       " (0.0, 'in shooting'),\n",
       " (0.0, 'of mass'),\n",
       " (0.0, 'in this'),\n",
       " (0.0, 'shootings in')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefs_and_features[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d07ca187",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.0, 'shootings obama'),\n",
       " (0.0, 'of 10,000'),\n",
       " (0.0, 'like a'),\n",
       " (0.0, 'now of'),\n",
       " (0.0, '@berniesanders in'),\n",
       " (0.0, 'have a'),\n",
       " (0.0, 'in shooting'),\n",
       " (0.0, 'of mass'),\n",
       " (0.0, 'in this'),\n",
       " (0.0, 'shootings in')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefs_and_features[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1bbd1712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mass shootings', 'charlie hebdo', 'lindt cafe', 'he was', '#charliehebdo attackers', 'will be', 'a rainbow', 'parliament hill', 'is not', 'red cross']\n"
     ]
    }
   ],
   "source": [
    "n_best = 750\n",
    "best_tokens = []\n",
    "\n",
    "# for cf in coefs_and_features[-n_best:]:\n",
    "#     best_tokens.append(cf[1])\n",
    "\n",
    "for cf in coefs_and_features[:n_best]:\n",
    "    best_tokens.append(cf[1])\n",
    "    \n",
    "print(best_tokens[:10])\n",
    "    \n",
    "with open(\"../../data/processed/twitter16-tf_best_terms.txt\", \"w\") as f:\n",
    "    for token in best_tokens:\n",
    "        f.write(token + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a333226",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
