{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "feff3561",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '../..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8a6e983",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>label</th>\n",
       "      <th>tvt</th>\n",
       "      <th>cv_fold</th>\n",
       "      <th>tt</th>\n",
       "      <th>tvt2</th>\n",
       "      <th>tvt2_1</th>\n",
       "      <th>tvt2_2</th>\n",
       "      <th>tvt2_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>731166399389962242</td>\n",
       "      <td>ðŸ”¥ca kkk grand wizard ðŸ”¥ endorses @hillaryclinto...</td>\n",
       "      <td>unverified</td>\n",
       "      <td>training</td>\n",
       "      <td>1</td>\n",
       "      <td>training</td>\n",
       "      <td>training</td>\n",
       "      <td>validation</td>\n",
       "      <td>training</td>\n",
       "      <td>training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>714598641827246081</td>\n",
       "      <td>an open letter to trump voters from his top st...</td>\n",
       "      <td>unverified</td>\n",
       "      <td>training</td>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "      <td>training</td>\n",
       "      <td>testting</td>\n",
       "      <td>training</td>\n",
       "      <td>training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>691809004356501505</td>\n",
       "      <td>america is a nation of second chances â€”@potus ...</td>\n",
       "      <td>non-rumor</td>\n",
       "      <td>training</td>\n",
       "      <td>2</td>\n",
       "      <td>training</td>\n",
       "      <td>validation</td>\n",
       "      <td>validation</td>\n",
       "      <td>validation</td>\n",
       "      <td>testting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>693204708933160960</td>\n",
       "      <td>brandon marshall visits and offers advice, sup...</td>\n",
       "      <td>non-rumor</td>\n",
       "      <td>training</td>\n",
       "      <td>1</td>\n",
       "      <td>training</td>\n",
       "      <td>testting</td>\n",
       "      <td>testting</td>\n",
       "      <td>training</td>\n",
       "      <td>training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>551099691702956032</td>\n",
       "      <td>rip elly may clampett: so sad to learn #beverl...</td>\n",
       "      <td>true</td>\n",
       "      <td>training</td>\n",
       "      <td>3</td>\n",
       "      <td>training</td>\n",
       "      <td>validation</td>\n",
       "      <td>training</td>\n",
       "      <td>validation</td>\n",
       "      <td>training</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id                                         tweet_text  \\\n",
       "0  731166399389962242  ðŸ”¥ca kkk grand wizard ðŸ”¥ endorses @hillaryclinto...   \n",
       "1  714598641827246081  an open letter to trump voters from his top st...   \n",
       "2  691809004356501505  america is a nation of second chances â€”@potus ...   \n",
       "3  693204708933160960  brandon marshall visits and offers advice, sup...   \n",
       "4  551099691702956032  rip elly may clampett: so sad to learn #beverl...   \n",
       "\n",
       "        label       tvt  cv_fold        tt        tvt2      tvt2_1  \\\n",
       "0  unverified  training        1  training    training  validation   \n",
       "1  unverified  training        1      test    training    testting   \n",
       "2   non-rumor  training        2  training  validation  validation   \n",
       "3   non-rumor  training        1  training    testting    testting   \n",
       "4        true  training        3  training  validation    training   \n",
       "\n",
       "       tvt2_2    tvt2_3  \n",
       "0    training  training  \n",
       "1    training  training  \n",
       "2  validation  testting  \n",
       "3    training  training  \n",
       "4  validation  training  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"../../data/processed/twitter15_dataset_with_tvt.csv\", lineterminator=\"\\n\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63153167",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>label</th>\n",
       "      <th>tvt</th>\n",
       "      <th>cv_fold</th>\n",
       "      <th>tt</th>\n",
       "      <th>tvt2</th>\n",
       "      <th>tvt2_1</th>\n",
       "      <th>tvt2_2</th>\n",
       "      <th>tvt2_3</th>\n",
       "      <th>label_rnr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>731166399389962242</td>\n",
       "      <td>ðŸ”¥ca kkk grand wizard ðŸ”¥ endorses @hillaryclinto...</td>\n",
       "      <td>unverified</td>\n",
       "      <td>training</td>\n",
       "      <td>1</td>\n",
       "      <td>training</td>\n",
       "      <td>training</td>\n",
       "      <td>validation</td>\n",
       "      <td>training</td>\n",
       "      <td>training</td>\n",
       "      <td>rumours</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>714598641827246081</td>\n",
       "      <td>an open letter to trump voters from his top st...</td>\n",
       "      <td>unverified</td>\n",
       "      <td>training</td>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "      <td>training</td>\n",
       "      <td>testting</td>\n",
       "      <td>training</td>\n",
       "      <td>training</td>\n",
       "      <td>rumours</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>691809004356501505</td>\n",
       "      <td>america is a nation of second chances â€”@potus ...</td>\n",
       "      <td>non-rumor</td>\n",
       "      <td>training</td>\n",
       "      <td>2</td>\n",
       "      <td>training</td>\n",
       "      <td>validation</td>\n",
       "      <td>validation</td>\n",
       "      <td>validation</td>\n",
       "      <td>testting</td>\n",
       "      <td>non-rumours</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>693204708933160960</td>\n",
       "      <td>brandon marshall visits and offers advice, sup...</td>\n",
       "      <td>non-rumor</td>\n",
       "      <td>training</td>\n",
       "      <td>1</td>\n",
       "      <td>training</td>\n",
       "      <td>testting</td>\n",
       "      <td>testting</td>\n",
       "      <td>training</td>\n",
       "      <td>training</td>\n",
       "      <td>non-rumours</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>551099691702956032</td>\n",
       "      <td>rip elly may clampett: so sad to learn #beverl...</td>\n",
       "      <td>true</td>\n",
       "      <td>training</td>\n",
       "      <td>3</td>\n",
       "      <td>training</td>\n",
       "      <td>validation</td>\n",
       "      <td>training</td>\n",
       "      <td>validation</td>\n",
       "      <td>training</td>\n",
       "      <td>rumours</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id                                         tweet_text  \\\n",
       "0  731166399389962242  ðŸ”¥ca kkk grand wizard ðŸ”¥ endorses @hillaryclinto...   \n",
       "1  714598641827246081  an open letter to trump voters from his top st...   \n",
       "2  691809004356501505  america is a nation of second chances â€”@potus ...   \n",
       "3  693204708933160960  brandon marshall visits and offers advice, sup...   \n",
       "4  551099691702956032  rip elly may clampett: so sad to learn #beverl...   \n",
       "\n",
       "        label       tvt  cv_fold        tt        tvt2      tvt2_1  \\\n",
       "0  unverified  training        1  training    training  validation   \n",
       "1  unverified  training        1      test    training    testting   \n",
       "2   non-rumor  training        2  training  validation  validation   \n",
       "3   non-rumor  training        1  training    testting    testting   \n",
       "4        true  training        3  training  validation    training   \n",
       "\n",
       "       tvt2_2    tvt2_3    label_rnr  \n",
       "0    training  training      rumours  \n",
       "1    training  training      rumours  \n",
       "2  validation  testting  non-rumours  \n",
       "3    training  training  non-rumours  \n",
       "4  validation  training      rumours  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_rnr = []\n",
    "for i, d in data.iterrows():\n",
    "    if d['label'] in [\"unverified\", \"true\", \"false\"]:\n",
    "        label_rnr.append(\"rumours\")\n",
    "    else:\n",
    "        label_rnr.append(\"non-rumours\")\n",
    "        \n",
    "data['label_rnr'] = pd.Series(label_rnr)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b95fb4a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['unverified', 'non-rumor', 'true', 'false']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# labels_str = ['rumour', 'non-rumour']\n",
    "label_type = \"label\"\n",
    "labels_str = data[label_type].unique().tolist()\n",
    "labels_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6ce7b52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 1, 1, 2, 1, 0, 2, 0, 3]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = []\n",
    "for i, d in data.iterrows():\n",
    "    lab = labels_str.index(d[label_type])\n",
    "    labels.append(lab)\n",
    "labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fc83a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tokenizer = TweetTokenizer(reduce_len=True)\n",
    "\n",
    "\n",
    "def text2unigrams(text):\n",
    "    texts = tokenizer.tokenize(text.encode('ascii', 'ignore').decode('utf8'))\n",
    "    texts = [t for t in texts if t not in string.punctuation]\n",
    "    texts = [t for t in texts if t not in ['URL', 'â€˜', 'â€™']]\n",
    "    \n",
    "    unigrams = texts\n",
    "    \n",
    "    return unigrams\n",
    "\n",
    "\n",
    "def text2bigrams(text):\n",
    "    texts = tokenizer.tokenize(text.encode('ascii', 'ignore').decode('utf8'))\n",
    "    texts = [t for t in texts if t not in string.punctuation]\n",
    "    texts = [t for t in texts if t not in ['URL', 'â€˜', 'â€™']]\n",
    "    \n",
    "    bigrams = nltk.bigrams(texts)\n",
    "    bigrams = map(' '.join, bigrams)\n",
    "    bigrams = [bgr for bgr in bigrams]\n",
    "    \n",
    "    return bigrams\n",
    "\n",
    "\n",
    "def text2trigrams(text):\n",
    "    texts = tokenizer.tokenize(text.encode('ascii', 'ignore').decode('utf8'))\n",
    "    texts = [t for t in texts if t not in string.punctuation]\n",
    "    texts = [t for t in texts if t not in ['URL', 'â€˜', 'â€™']]\n",
    "    \n",
    "    trigrams = nltk.trigrams(texts)\n",
    "    trigrams = map(' '.join, trigrams)\n",
    "    trigrams = [bgr for bgr in trigrams]\n",
    "    \n",
    "    return trigrams\n",
    "\n",
    "\n",
    "def bigrams_vectors_generation(texts):\n",
    "    bigram_vectors = []\n",
    "    for text in texts:\n",
    "        bigrams = text2bigrams(text)\n",
    "\n",
    "        init_vec = [0.0 for _ in range(len(bigram_vector_base) + 1)]\n",
    "        for bgr in bigrams:\n",
    "            if bgr in bigram_vector_base:\n",
    "                idx = bigram_vector_base.index(bgr)\n",
    "                init_vec[idx] = 1.0\n",
    "            else:\n",
    "                init_vec[-1] = 1.0\n",
    "        bigram_vectors.append(init_vec)\n",
    "    \n",
    "    return bigram_vectors\n",
    "\n",
    "\n",
    "def custom_vectors_generation(texts, vector_terms):\n",
    "    vectors = []\n",
    "    for text in texts:\n",
    "        bigrams = text2bigrams(text)\n",
    "        trigrams = text2trigrams(text)\n",
    "\n",
    "        init_vec = [0.0 for _ in range(len(vector_terms) + 1)]\n",
    "        for bgr in bigrams:\n",
    "            if bgr in vector_terms:\n",
    "                idx = vector_terms.index(bgr)\n",
    "                init_vec[idx] = 1.0\n",
    "            else:\n",
    "                init_vec[-1] = 1.0\n",
    "        for tgr in trigrams:\n",
    "            if tgr in vector_terms:\n",
    "                idx = vector_terms.index(tgr)\n",
    "                init_vec[idx] = 1.0\n",
    "            else:\n",
    "                init_vec[-1] = 1.0\n",
    "        vectors.append(init_vec)\n",
    "    \n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97281e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = data['tweet_text'].tolist()\n",
    "# vectors = bigrams_vectors_generation(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31e1c006",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.collocations import *\n",
    "\n",
    "top_n = 2000\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "\n",
    "finder2 = BigramCollocationFinder.from_words([])\n",
    "finder3 = TrigramCollocationFinder.from_words([])\n",
    "\n",
    "# generating bigram and trigram\n",
    "for text in texts:\n",
    "    unigrams = text2unigrams(text)\n",
    "    bigrams = text2bigrams(text)\n",
    "    trigrams = text2trigrams(text)\n",
    "    \n",
    "    for ngrm in unigrams:\n",
    "        if ngrm not in finder2.word_fd:\n",
    "            finder2.word_fd[ngrm] = 0\n",
    "        finder2.word_fd[ngrm] += 1\n",
    "        finder2.N += 1\n",
    "        \n",
    "        if ngrm not in finder3.word_fd:\n",
    "            finder3.word_fd[ngrm] = 0\n",
    "        finder3.word_fd[ngrm] += 1\n",
    "        finder3.N += 1\n",
    "    \n",
    "    for ngrm in bigrams:\n",
    "        term = tuple([i for i in ngrm.split()])\n",
    "        \n",
    "        if term not in finder2.ngram_fd:\n",
    "            finder2.ngram_fd[term] = 0\n",
    "            \n",
    "        finder2.ngram_fd[term] += 1\n",
    "\n",
    "    for ngrm in trigrams:\n",
    "        term = tuple([i for i in ngrm.split()])\n",
    "        \n",
    "        if term not in finder3.ngram_fd:\n",
    "            finder3.ngram_fd[term] = 0\n",
    "            \n",
    "        finder3.ngram_fd[term] += 1\n",
    "        \n",
    "# only bigrams that appear 3+ times\n",
    "finder2.apply_freq_filter(3)\n",
    "finder3.apply_freq_filter(3)\n",
    "\n",
    "combined = []\n",
    "for res in finder2.score_ngrams(bigram_measures.pmi):\n",
    "    combined.append(res)\n",
    "for res in finder3.score_ngrams(trigram_measures.pmi):\n",
    "    combined.append(res)\n",
    "combined = sorted(combined, key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "703b0f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1081 542\n",
      "('kim', 'jong', 'un') - 25.56203625193502\n",
      "('leaving', 'fifty', 'shades') - 25.56203625193502\n",
      "('pro', 'era', 't-shirt') - 25.56203625193502\n",
      "('abdel-majed', 'abdel', 'bary') - 24.731961253377328\n",
      "('walmart', 'donates', '10,000') - 23.994995659211124\n",
      "('ios', '8', 'bug') - 23.509568832040884\n",
      "('cutting', '5,000', 'jobs') - 23.410033158489966\n",
      "('teen', 'leaves', 'heartbreaking') - 23.27252963474003\n",
      "('doritos', 'flavored', 'mountain') - 23.146998752656174\n",
      "(\"putin's\", 'motorcade', 'posted') - 23.146998752656174\n"
     ]
    }
   ],
   "source": [
    "print(len(finder2.ngram_fd), len(finder3.ngram_fd))\n",
    "count = 0\n",
    "for k, v in combined:\n",
    "    print(f\"{k} - {v}\")\n",
    "    count += 1\n",
    "    if count >= 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e3377a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "term_vector_base = [\" \".join(c[0]) for c in combined[:top_n]]\n",
    "vectors = custom_vectors_generation(texts, term_vector_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66934ad8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1624"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectors[102])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e2b1180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.27523148810644216\n",
      "25.56203625193502\n",
      "1081\n",
      "542\n"
     ]
    }
   ],
   "source": [
    "min_score = 100\n",
    "max_score = 0\n",
    "n_2gram = 0\n",
    "n_3gram = 0\n",
    "for k, v in combined[:top_n]:\n",
    "    min_score = min(min_score, v)\n",
    "    max_score = max(max_score, v)\n",
    "    \n",
    "    if len(k) == 2:\n",
    "        n_2gram += 1\n",
    "    \n",
    "    if len(k) == 3:\n",
    "        n_3gram += 1\n",
    "        \n",
    "print(min_score)\n",
    "print(max_score)\n",
    "print(n_2gram)\n",
    "print(n_3gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8cccd2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vectors = np.array([vectors[i] for i, d in data.iterrows() if d['tvt2'] == 'training'])\n",
    "val_vectors = np.array([vectors[i] for i, d in data.iterrows() if d['tvt2'] == 'validation'])\n",
    "test_vectors = np.array([vectors[i] for i, d in data.iterrows() if d['tvt2'] == 'testting'])\n",
    "\n",
    "train_labels = np.array([labels[i] for i, d in data.iterrows() if d['tvt2'] == 'training'])\n",
    "val_labels = np.array([labels[i] for i, d in data.iterrows() if d['tvt2'] == 'validation'])\n",
    "test_labels = np.array([labels[i] for i, d in data.iterrows() if d['tvt2'] == 'testting'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "81824aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- START ---\n",
      "---> execution time : 0.08 seconds\n",
      "Validation Set\n",
      "338 vs 338\n",
      "Multi Class Evaluation\n",
      "\n",
      "Class unverified Evaluation\n",
      "- Precision : 80.702 %\n",
      "- Recall : 54.118 %\n",
      "- F1 : 0.64789\n",
      "\n",
      "Class non-rumor Evaluation\n",
      "- Precision : 54.135 %\n",
      "- Recall : 80.0 %\n",
      "- F1 : 0.64574\n",
      "\n",
      "Class true Evaluation\n",
      "- Precision : 90.667 %\n",
      "- Recall : 80.0 %\n",
      "- F1 : 0.85\n",
      "\n",
      "Class false Evaluation\n",
      "- Precision : 71.233 %\n",
      "- Recall : 66.667 %\n",
      "- F1 : 0.68874\n",
      "\n",
      "Combined Evaluation\n",
      "- Accuracy : 70.414 %\n",
      "- Precision : 74.184 %\n",
      "- Recall : 70.196 %\n",
      "- F1 : 0.72135\n",
      "\n",
      "- Average Confidence : 100.0 %\n",
      "Model, Combined,,,,unverified,,,non-rumor,,,true,,,false,,,\n",
      "Anonymous, 70.414, 74.184, 70.196, 0.72135, 80.702, 54.118, 0.64789, 54.135, 80.0, 0.64574, 90.667, 80.0, 0.85, 71.233, 66.667, 0.68874, \n",
      "--- END ---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from library.classification import SKLearnClassification\n",
    "from library.evaluation import ConfusionMatrix\n",
    "\n",
    "svm = LinearSVC()\n",
    "\n",
    "model = SKLearnClassification(svm, \"Support Vector Machine\")\n",
    "print(f\"\\n--- START ---\")\n",
    "model.train(train_vectors, train_labels, \"Phemernr2\")\n",
    "\n",
    "print(\"Validation Set\")\n",
    "preds = model.predict(val_vectors)\n",
    "\n",
    "conf_mat = ConfusionMatrix(\n",
    "    labels=np.array([[1 if j == v else 0 for j in range(len(labels_str))] for v in val_labels]),\n",
    "    predictions=np.array([[1 if j == p else 0 for j in range(len(labels_str))] for p in preds]),\n",
    "    binary=False\n",
    ")\n",
    "conf_mat.evaluate(labels_str)\n",
    "\n",
    "print(\"--- END ---\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "33a6804c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens : 1623\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "coefs = model.model.coef_\n",
    "if type(coefs) == csr_matrix:\n",
    "    coefs.toarray().tolist()[0]\n",
    "else:\n",
    "    coefs.tolist()\n",
    "coefs_and_features = list(zip(coefs[0], term_vector_base))\n",
    "\n",
    "# Most predictive overall\n",
    "# coefs_and_features = sorted(coefs_and_features, key=lambda x: x[0], reverse=True)\n",
    "coefs_and_features = sorted(coefs_and_features, key=lambda x: abs(x[0]), reverse=True)\n",
    "print(f\"Total tokens : {len(coefs_and_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c865ec2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 1624)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5bc37609",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1.3361089048568273, '#ferguson pd'),\n",
       " (1.2727333020425338, 'bathroom policy'),\n",
       " (1.147679665470529, 'institutional racism'),\n",
       " (1.119902012183137, 'want to'),\n",
       " (1.1062405807581122, 'bag charge'),\n",
       " (1.1003248690153882, 'ios 8'),\n",
       " (1.0391957445022628, 'can be'),\n",
       " (1.039195734771796, 'clinton campaign'),\n",
       " (1.0391957183250509, 'new transgender'),\n",
       " (1.01127797326147, 'protest at')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefs_and_features[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d07ca187",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.0, 'lego to'),\n",
       " (0.0, 'soldier at'),\n",
       " (0.0, 'the canadian'),\n",
       " (0.0, 'a canadian'),\n",
       " (0.0, 'a spider'),\n",
       " (0.0, 'is this'),\n",
       " (0.0, 'the shooting'),\n",
       " (0.0, 'died of'),\n",
       " (0.0, 'not a'),\n",
       " (0.0, 'to paul')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefs_and_features[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1bbd1712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#ferguson pd', 'bathroom policy', 'institutional racism', 'want to', 'bag charge', 'ios 8', 'can be', 'clinton campaign', 'new transgender', 'protest at']\n"
     ]
    }
   ],
   "source": [
    "n_best = 750\n",
    "best_tokens = []\n",
    "\n",
    "# for cf in coefs_and_features[-n_best:]:\n",
    "#     best_tokens.append(cf[1])\n",
    "\n",
    "for cf in coefs_and_features[:n_best]:\n",
    "    best_tokens.append(cf[1])\n",
    "    \n",
    "print(best_tokens[:10])\n",
    "    \n",
    "with open(\"../../data/processed/twitter15-multi_best_terms.txt\", \"w\") as f:\n",
    "    for token in best_tokens:\n",
    "        f.write(token + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a333226",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
